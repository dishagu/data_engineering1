{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "800a2b55-6349-42e5-9ea0-878ba04bc4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hdfs in c:\\users\\manoj\\anaconda3\\lib\\site-packages (2.7.3)\n",
      "Requirement already satisfied: docopt in c:\\users\\manoj\\anaconda3\\lib\\site-packages (from hdfs) (0.6.2)\n",
      "Requirement already satisfied: requests>=2.7.0 in c:\\users\\manoj\\anaconda3\\lib\\site-packages (from hdfs) (2.31.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\manoj\\anaconda3\\lib\\site-packages (from hdfs) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\manoj\\anaconda3\\lib\\site-packages (from requests>=2.7.0->hdfs) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\manoj\\anaconda3\\lib\\site-packages (from requests>=2.7.0->hdfs) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\manoj\\anaconda3\\lib\\site-packages (from requests>=2.7.0->hdfs) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\manoj\\anaconda3\\lib\\site-packages (from requests>=2.7.0->hdfs) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde95ec0-bedc-4791-9315-af57f1a58278",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb4c2aab-3b3e-4246-8aba-4b3e84f7b8b3",
   "metadata": {},
   "source": [
    "### Script to export the data from local to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b5fedca-6810-47f5-be80-45aa2c7fd488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded folder C:/Users/Manoj/Downloads/DE_SCRAPPING to /home/hadoop/data/nameNode\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "# HDFS client\n",
    "hdfs_client = InsecureClient('http://100.126.132.104:9870', user='hadoop')\n",
    "\n",
    "# Path to the folder containing files\n",
    "local_data_folder = 'C:/Users/Manoj/Downloads/DE_SCRAPPING'\n",
    "# Path in HDFS to store the files\n",
    "hdfs_target_folder = '/home/hadoop/data/nameNode'\n",
    "\n",
    "# Ensuring the HDFS target folder exists\n",
    "hdfs_client.makedirs(hdfs_target_folder)\n",
    "\n",
    "# Uploading the entire folder to HDFS \n",
    "try:\n",
    "    hdfs_client.upload(hdfs_target_folder, local_data_folder, overwrite=True)\n",
    "    print(f'Successfully uploaded folder {local_data_folder} to {hdfs_target_folder}')\n",
    "except Exception as e:\n",
    "    print(f'Failed to upload folder {local_data_folder}: {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6a7ce5-95d6-4778-9409-a384cae09470",
   "metadata": {},
   "source": [
    "### Script to ingest the data to SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ef0de22d-73a2-4968-a6ff-5e08728526c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully inserted data into TeamData\n",
      "Successfully inserted data into PlayerInfo\n",
      "Successfully inserted data into GroundInfo\n",
      "Successfully inserted data into MatchSummary\n",
      "Successfully inserted data into MatchDetails\n",
      "Successfully inserted data into BattingSummary\n",
      "Successfully inserted data into BowlingSummary\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from hdfs import InsecureClient\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "\n",
    "# HDFS connection detailsv (master-node,user)\n",
    "hdfs_host = 'http://100.126.132.104:9870'\n",
    "hdfs_user = 'hadoop'\n",
    "hdfs_client = InsecureClient(hdfs_host, user=hdfs_user)\n",
    "\n",
    "# List of tables and their corresponding HDFS paths\n",
    "hdfs_directory = \"/home/hadoop/data/nameNode/DE_SCRAPPING/\"\n",
    "tables = {\n",
    "    'TeamData': hdfs_directory + 'teamData.csv',\n",
    "    'PlayerInfo': hdfs_directory + 'playerinformation.csv',\n",
    "    'GroundInfo': hdfs_directory + 'groundDetails.csv',\n",
    "    'MatchSummary': hdfs_directory + 'matchsummary.csv',\n",
    "    'MatchDetails': hdfs_directory + 'matchDetails.csv',\n",
    "    'BattingSummary': hdfs_directory + 'battingSummary.csv',\n",
    "    'BowlingSummary': hdfs_directory + 'bowlingsummary.csv',\n",
    "}\n",
    "\n",
    "# MySQL database connection details\n",
    "db_username = 'root'\n",
    "db_password = '123456'\n",
    "db_host = '192.168.56.101'\n",
    "db_port = '3306'  # Default MySQL port\n",
    "db_name = 't20'\n",
    "\n",
    "# Create a connection string based on the details above\n",
    "connection_string = f'mysql+pymysql://{db_username}:{db_password}@{db_host}:{db_port}/{db_name}'\n",
    "\n",
    "# Create an SQLAlchemy engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Function to read CSV from HDFS and convert to Pandas DataFrame\n",
    "def read_hdfs_csv(hdfs_path):\n",
    "    with hdfs_client.read(hdfs_path, encoding='utf-8') as reader:\n",
    "        return pd.read_csv(reader)\n",
    "\n",
    "# Iterate over the tables dictionary\n",
    "for table_name, hdfs_path in tables.items():\n",
    "    try:\n",
    "        if hdfs_path.endswith('.csv'):\n",
    "            # Read CSV file from HDFS into a Pandas DataFrame\n",
    "            pandas_df = read_hdfs_csv(hdfs_path)\n",
    "        else:\n",
    "            print(f\"Unsupported file type: {hdfs_path}\")\n",
    "            continue\n",
    "\n",
    "        # Ingest data into SQL table\n",
    "        pandas_df.to_sql(name=table_name, con=engine, if_exists='append', index=False)\n",
    "        print(f'Successfully inserted data into {table_name}')\n",
    "    except Exception as e:\n",
    "        print(f'Failed to insert data into {table_name}: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7f0ede-1b79-4512-b7fb-ff22f122d6ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
